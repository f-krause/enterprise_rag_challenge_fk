{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experiments",
   "id": "8dee0d0f27a232f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:07:28.361081Z",
     "start_time": "2025-02-13T09:07:28.337681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "665e41a8c5781f91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:33:56.121076Z",
     "start_time": "2025-02-13T09:33:56.109331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# company = \"reit\"\n",
    "# company = \"yellow_pages\"\n",
    "company = \"calyxt\"\n",
    "\n",
    "test_pdf_path = f\"../data/eval/{company}.pdf\""
   ],
   "id": "9d2e24bb9833b8ef",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Docling - strong!",
   "id": "c2bb2e752d559444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T15:57:28.807692Z",
     "start_time": "2025-02-10T15:57:25.740499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Potential for parallelization for files?\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# source = \"https://arxiv.org/pdf/2408.09869\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(test_pdf_path)\n",
    "print(result.document.export_to_markdown())\n",
    "\n",
    "# REIT: execution for 7.4MB with 97 pages -> 5 min\n",
    "# Yellow Pages: execution for 1.4MB with 77 pages -> 4 min\n",
    "# Calyxt: execution for 0.7MB with 88 pages -> 3min"
   ],
   "id": "7eab7904ca084e9c",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Potential for parallelization for files?\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocument_converter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DocumentConverter\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# source = \"https://arxiv.org/pdf/2408.09869\"  # document per local path or URL\u001B[39;00m\n\u001B[0;32m      5\u001B[0m converter \u001B[38;5;241m=\u001B[39m DocumentConverter()\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniforge3\\envs\\rag_challenge\\lib\\site-packages\\docling\\document_converter.py:20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmspowerpoint_backend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MsPowerpointDocumentBackend\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmsword_backend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MsWordDocumentBackend\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpubmed_backend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PubMedDocumentBackend\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01muspto_backend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PatentUsptoDocumentBackend\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdocling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatamodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     23\u001B[0m     ConversionStatus,\n\u001B[0;32m     24\u001B[0m     DoclingComponentType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     27\u001B[0m     InputFormat,\n\u001B[0;32m     28\u001B[0m )\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1002\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:945\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(name, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1439\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1411\u001B[0m, in \u001B[0;36m_get_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1544\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(self, fullname, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:147\u001B[0m, in \u001B[0;36m_path_stat\u001B[1;34m(path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# domain knwoledge\n",
    "# brackets in tables indicate negative values\n",
    "# parsing issues (see current system prompt)"
   ],
   "id": "c9aa38b8c65214cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T13:22:53.397484Z",
     "start_time": "2025-02-07T13:22:53.244857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# STORE as md with correct utf-8 encoding\n",
    "with open(f\"../dev/docling_{company}.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(result.document.export_to_markdown())"
   ],
   "id": "8bedf235d19dbd5d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Google Gemini",
   "id": "5e27cfdbedf89a26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:35:03.129594Z",
     "start_time": "2025-02-13T09:34:56.945603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "from google import genai\n",
    "\n",
    "model_id = \"gemini-2.0-flash\"  # or \"gemini-2.0-flash-lite-preview-02-05\"  , \"gemini-2.0-pro-exp-02-05\", at least 1M token input context window\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "response = client.models.generate_content(\n",
    "    model=model_id, contents=\"What is your maximum input window size? What are the free limits for parsing PDFs?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "id": "6bbd1c2855f969cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down my input limitations and how I handle PDFs.\n",
      "\n",
      "**Maximum Input Window Size**\n",
      "\n",
      "My maximum input window size, or context window, is currently **200,000 tokens.**\n",
      "\n",
      "*   **What does this mean?**  This refers to the amount of text I can \"remember\" and consider at one time when processing a request. A \"token\" is roughly equivalent to a word or part of a word. So, 200,000 tokens translates to roughly 150,000 words, but this can vary depending on the specific text.\n",
      "*   **Why is there a limit?** Processing large amounts of text requires significant computational resources.  The context window limit balances the ability to handle complex tasks with efficiency.\n",
      "\n",
      "**Free Limits for Parsing PDFs**\n",
      "\n",
      "There are no inherent limits on the size or number of PDFs that you can ask me questions about as long as the content of the PDFs, when extracted as text, fits within my 200,000 token context window.\n",
      "\n",
      "*   **How it works:** I don't directly \"read\" PDFs in their native binary format.  Instead, you (or an intermediary tool) need to *extract the text* from the PDF and provide that text as my input.\n",
      "*   **Important considerations**\n",
      "    *   The text extracted from the PDF must fit into the context window.\n",
      "    *   I am not the best tool for OCR/document conversion. If the PDFs contain images or scanned text, you'll first need to use a dedicated OCR (Optical Character Recognition) tool to convert the images into machine-readable text before providing it to me.\n",
      "    *   PDFs with complex layouts (multiple columns, tables, images embedded within text) *may* result in poorly formatted or incomplete text extraction, which could affect the accuracy of my responses.  You might need to do some manual cleanup of the extracted text.\n",
      "*   **How to handle large PDFs:**\n",
      "    1.  **Split the PDF:** If the text content of the PDF exceeds my context window, you'll need to split it into smaller chunks (e.g., by chapter, section, or page ranges).\n",
      "    2.  **Process in sequence:**  Provide the text chunks to me one at a time, perhaps asking questions about each chunk individually.\n",
      "    3.  **Summarization and focused questions:**  You can initially ask me to summarize sections of the PDF and then follow up with more detailed questions about the summarized content. This way, you're feeding me smaller, more relevant pieces of information at a time.\n",
      "\n",
      "**In Summary**\n",
      "\n",
      "*   I have a 200,000 token input window.\n",
      "*   There are no limitations to using me for PDFs as long as the extracted text from the PDF fits within the input window size.\n",
      "*   You are responsible for extracting the text from the PDF and ensuring its quality.\n",
      "*   For large PDFs, split them into smaller chunks or use summarization techniques.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:34:37.392683Z",
     "start_time": "2025-02-13T09:34:35.870239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upload PDF\n",
    "invoice_pdf = client.files.upload(file=test_pdf_path, config={'display_name': f'{company}_annual_report'})"
   ],
   "id": "b5ba7c60104db029",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:35:11.262622Z",
     "start_time": "2025-02-13T09:35:03.194821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_size = client.models.count_tokens(model=model_id, contents=invoice_pdf)\n",
    "print(f'File: {invoice_pdf.display_name} equals to {file_size.total_tokens} tokens')"
   ],
   "id": "ae662f96e7629a2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: calyxt_annual_report equals to 103341 tokens\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:56:26.727120Z",
     "start_time": "2025-02-13T09:56:26.702502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Metric(str, Enum):\n",
    "    total_revenue = \"total revenue\"\n",
    "    net_income = \"net income\"\n",
    "    total_liabilities = \"total liabilities\"\n",
    "    total_assets = \"total assets\"\n",
    "\n",
    "    # rad_expenses = \"research and development expenses\"\n",
    "    # risk_management_spending = \"risk management spending\"\n",
    "    # debt_to_equity_ratio = \"Debt-To-Equity ratio\"\n",
    "    # number_of_stores = \"Number of stores\"\n",
    "    # return_on_assets = \"Return on Assets (ROA)\"\n",
    "    # return_on_equity = \"Return on Equity (ROE)\"\n",
    "    # customer_acquisition_spending = \"customer acquisition spending\"\n",
    "    # operating_margin = \"operating margin\"\n",
    "    # market_capitalization = \"market capitalization\"\n",
    "    # sustainability_initiatives_spending = \"sustainability initiatives spending\"\n",
    "    # gross_profit_margin = \"gross profit margin\"\n",
    "    # net_profit_margin = \"net profit margin\"\n",
    "    # intangible_assets = \"intangible assets\"\n",
    "    # marketing_spending = \"marketing spending\"\n",
    "    # free_cash_flow = \"free cash flow\"\n",
    "    # earnings_per_share = \"earnings per share (EPS)\"\n",
    "    # accounts_receivable = \"accounts_receivable\"\n",
    "    # acquisition_costs = \"acquisition costs\"\n",
    "    # shareholders_equity = \"shareholders' equity\"\n",
    "    # operating_cash_flow = \"operating cash flow\"\n",
    "    # quick_ratio = \"Quick Ratio\"\n",
    "    # inventory = \"inventory\"\n",
    "\n",
    "\n",
    "class DocumentDataPoint(BaseModel):\n",
    "    metric_type: Metric = Field(..., title=\"One of the possible metric types\")\n",
    "    chain_of_thought: str = Field(..., title=\"Chain of thought leading to the metric\")\n",
    "    value: float = Field(..., title=\"value according\")\n",
    "    point_in_time_as_iso_date: Optional[str] = Field(..., title=\"Point in time of the metric as ISO date\")\n",
    "\n",
    "\n",
    "class DocumentContent(BaseModel):\n",
    "    data_points: list[DocumentDataPoint] = Field(..., title=\"Data points extracted from the document\")  # according to guidelines\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "\"You will be prompted with the contents of a document. Your task is to extract various metrics \"\n",
    "# \"as well as company role assignments \"\n",
    "\"from this document. With each metric, supply the point in \"\n",
    "\"time when the metric was measured according to the document.\"\n",
    "# \"as well as the currency (if applicable). \"\n",
    "\"If the metric is given in some unit, convert it to the exact amount (e.g. \"\n",
    "\"if the amount in the document is given as '100 (in thousands)' \"\n",
    "\"or '100k', insert the full value '100000').\"\n",
    "# \"With each role assignment, supply when the role assignment started and ended, if possible.\"\n",
    "\"\\n\\n\"\n",
    "\"Do your best to include as many metrics for as many points in time as possible!\""
   ],
   "id": "15e7963006f4c004",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do your best to include as many metrics for as many points in time as possible!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:56:22.454431Z",
     "start_time": "2025-02-13T09:56:22.440586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple start\n",
    "class DocumentContent(BaseModel):\n",
    "    company_name: str = Field(..., title=\"Name of the company\")\n",
    "    year: int = Field(..., title=\"Year of the annual report\")\n",
    "\n",
    "prompt = \"You are an assistant to find out the company name and year of the document provided. \""
   ],
   "id": "cc597cae71bf31d6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:59:24.187939Z",
     "start_time": "2025-02-13T09:59:24.176965Z"
    }
   },
   "cell_type": "code",
   "source": "response.parsed.data_points[0]",
   "id": "4625464926838410",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentDataPoint(metric_type=<Metric.total_assets: 'total assets'>, chain_of_thought='The consolidated balance sheets list the total assets. The value of the total assets is the last value of the column for asset side.', value=22.421, point_in_time_as_iso_date='2022-12-31')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T09:56:52.822887Z",
     "start_time": "2025-02-13T09:56:37.050605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate a response using the Person model\n",
    "# response = client.models.generate_content(model=model_id, contents=prompt,\n",
    "#                                           config={'response_mime_type': 'application/json',\n",
    "#                                                   'response_schema': DocumentContent})\n",
    "\n",
    "response = client.models.generate_content(model=model_id, contents=[prompt, invoice_pdf], config={'response_mime_type': 'application/json', 'response_schema': DocumentContent})\n",
    "\n",
    "for data_point in response.parsed.data_points:\n",
    "    print(data_point)"
   ],
   "id": "f224c1202cdca4b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_type=<Metric.total_assets: 'total assets'> chain_of_thought='The consolidated balance sheets list the total assets. The value of the total assets is the last value of the column for asset side.' value=22.421 point_in_time_as_iso_date='2022-12-31'\n",
      "metric_type=<Metric.total_liabilities: 'total liabilities'> chain_of_thought='The consolidated balance sheets list the total liabilities. The value of the total liabilities is the last value of the liabilities side.' value=15.188 point_in_time_as_iso_date='2022-12-31'\n",
      "metric_type=<Metric.net_income: 'net income'> chain_of_thought='The consolidated statements of operations list the net loss which will be converted to net income. From the image I found the value for net loss and change it from negative to positive to get net income.' value=16.891 point_in_time_as_iso_date='2022-12-31'\n",
      "metric_type=<Metric.total_revenue: 'total revenue'> chain_of_thought='The consolidated statements of operations list the revenue. The total revenue is presented on a thousands basis.' value=157000.0 point_in_time_as_iso_date='2022-12-31'\n",
      "metric_type=<Metric.total_revenue: 'total revenue'> chain_of_thought='The consolidated statements of operations list the revenue. The total revenue is presented on a thousands basis.' value=25987000.0 point_in_time_as_iso_date='2021-12-31'\n",
      "metric_type=<Metric.net_income: 'net income'> chain_of_thought='The consolidated statements of operations list the net loss which will be converted to net income. From the image I found the value for net loss and change it from negative to positive to get net income.' value=29199000.0 point_in_time_as_iso_date='2021-12-31'\n",
      "metric_type=<Metric.total_liabilities: 'total liabilities'> chain_of_thought='The consolidated balance sheets list the total liabilities. The value of the total liabilities is the last value of the liabilities side.' value=23062000.0 point_in_time_as_iso_date='2021-12-31'\n",
      "metric_type=<Metric.total_assets: 'total assets'> chain_of_thought='The consolidated balance sheets list the total assets. The value of the total assets is the last value of the column for asset side.' value=37194000.0 point_in_time_as_iso_date='2021-12-31'\n",
      "metric_type=<Metric.total_revenue: 'total revenue'> chain_of_thought='The consolidated statements of operations list the revenue. The total revenue is presented on a thousands basis.' value=23851000.0 point_in_time_as_iso_date='2020-12-31'\n",
      "metric_type=<Metric.net_income: 'net income'> chain_of_thought='The consolidated statements of operations list the net loss which will be converted to net income. From the image I found the value for net loss and change it from negative to positive to get net income.' value=44836000.0 point_in_time_as_iso_date='2020-12-31'\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Not working well",
   "id": "226c443d93dbeb39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PyMuPDF4LLM - tables bad",
   "id": "fd999db81fc5b25c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Observations\n",
    "# tables are pretty bad\n",
    "# format in general seems ok"
   ],
   "id": "bf396e4508bf37d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T12:09:47.042681Z",
     "start_time": "2025-02-07T12:09:47.035658Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6,
   "source": [
    "# Store as file\n",
    "import pathlib\n",
    "\n",
    "pathlib.Path(\"../dev/pymupdf4llm_output.md\").write_bytes(md_text.encode())"
   ],
   "id": "c0667d96b7b1ee99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T12:09:43.902152Z",
     "start_time": "2025-02-07T12:09:43.893538Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-----\\n\\n# Table of Contents\\n\\nManagement’s Discussion and Analysis........................................................................................................ 2\\n\\nIndependent Auditor’s Report ................................................................................................................. 29-32\\n\\nConsolidated Statements of Income and Other Comprehensive Income ..................................................... 33\\n\\nConsolidated Statements of Financial Position ............................................................................................ 34\\n\\nConsolidated Statements of Changes in Equity ...................................................................................... 35-36\\n\\nConsolidated Statements of Cash Flows ..................................................................................................... 37\\n\\nNotes To The Consolidated Financial Statements ..................................................................................'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5,
   "source": "md_text[:1000]",
   "id": "478c0d8dec0f8df4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Camelot (table detection) - Not working well",
   "id": "4cae618b6fdaf1ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import camelot\n",
    "\n",
    "# Extract tables from all pages of the PDF\n",
    "tables = camelot.read_pdf(test_pdf_path, pages=\"all\")\n",
    "\n",
    "# Export each table to CSV (or process further as needed)\n",
    "for i, table in enumerate(tables):\n",
    "    csv_filename = f\"table_{i}.csv\"\n",
    "    table.to_csv(csv_filename)\n",
    "    print(f\"Exported table {i} to {csv_filename}\")\n"
   ],
   "id": "edf1337d5be7e39d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
